<!doctype html>
<html class="no-js">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>CS766 Project</title>
        <meta name="author">
        <meta name="description" content="CS766 Deep object detection benchmarking and sensitivity analysis.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="generator" content="Hugo 0.36" />
        <link href="https://dcompgriff.github.io/ML_OD_Benchmarking/post/index.xml" rel="alternate" type="application/rss+xml" title="CS766 Project" />
        <link href="https://dcompgriff.github.io/ML_OD_Benchmarking/post/index.xml" rel="feed" type="application/rss+xml" title="CS766 Project" />
        <link href='//fonts.googleapis.com/css?family=Roboto:400,300,700|Noto+Serif:400,400italic,700,700italic' rel='stylesheet' type='text/css'>
        <link rel="stylesheet" href="https://dcompgriff.github.io/ML_OD_Benchmarking/css/styles.css">
        <link rel="icon" href="https://dcompgriff.github.io/ML_OD_Benchmarking/favicon.ico">
        <link rel="apple-touch-icon" href="https://dcompgriff.github.io/ML_OD_Benchmarking/apple-touch-icon.png" />
        <link rel="stylesheet" href="https://dcompgriff.github.io/ML_OD_Benchmarking/css/highlightjs/monokai.css">
        <script src="https://dcompgriff.github.io/ML_OD_Benchmarking/js/vendor/modernizr-2.8.0.min.js"></script>
        
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous"> -->
	<style>
        .site-header h2 .logo {
        background: url(https://dcompgriff.github.io/ML_OD_Benchmarking/img/desk.jpg) no-repeat 0 0;
        }
        @media (min--moz-device-pixel-ratio: 1.3), (-o-min-device-pixel-ratio: 2.6 / 2), (-webkit-min-device-pixel-ratio: 1.3), (min-device-pixel-ratio: 1.3), (min-resolution: 1.3dppx) {
          .site-header h2 .logo {
            background-image: url(https://dcompgriff.github.io/ML_OD_Benchmarking/img/desk.jpg);
        }}
       .site-header {
         background: #2a373d url(https://dcompgriff.github.io/ML_OD_Benchmarking/img/desk.jpg) no-repeat center center;
         z-index: -1;
        }
        </style>


    </head>
    <body>
        
        <header class="site-header">
          <div class="transparent-layer">
              <h2>CS 766 Project: Deep Object Detection Network Analysis.</h2>
          </div>
        </header>


<div class="container clearfix">
    <main role="main" class="content">
        <article class="post">
            <a class="btn home" href="https://dcompgriff.github.io/ML_OD_Benchmarking" title="Back to home">&laquo; Back to home</a>
            
<h1><a href="https://dcompgriff.github.io/ML_OD_Benchmarking/post/deep-object-detection-benchmarking/" title="Deep object detection network benchmarking and sensitivity analysis.">Deep object detection network benchmarking and sensitivity analysis.</a></h1>

<footer class="post-info">Posted on <span class="post-meta"><time datetime="2018.04.23">2018.04.23</time>

    

</span>
</footer>

            

<h2 id="introduction">Introduction</h2>

<p>Deep object detection methods represent the pinnacle of cutting edge object detection methods.</p>

<h2 id="background">Background</h2>

<h5 id="object-detection">Object Detection</h5>

<p>Dealing with object in images can loosely be broken down into 4 main categories (at the time of making this post anyways).
The first method of dealing with objects in images is &ldquo;Object Classification&rdquo;. In this task, an entire image is taken to
represent a single kind of object, and the goal is to classify the image. The next, more advanced task is Object Classification
with &ldquo;Localization&rdquo;. In this task, we try to determine two things. The first is where the object is within an image, and the second
is to classigy what object is in that location. The next task is &ldquo;Object Detection&rdquo;. This builds on Object Classification
with Localization by finding multiple regions likely to have an object, then classifying each region as an individual object. The
final task is &ldquo;Instance Segmentation&rdquo; (Image Segmentation), where the meaningful objects within an image are organized into
singular segments. For our project, we focus on the &ldquo;Object Detection&rdquo; task. A visual representation of the different kinds of
tasks is provided below.</p>

<p><strong>Object Detection Tasks</strong>
<img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/obj_det_kinds.png" alt="Object Detection Kinds" title="Object Detection Tasks" /></p>

<p>As stated above, the Object Detection task is comprised of two parts. The first part deals with generating what are called
&ldquo;Object Proposals&rdquo;. Object Proposals are regions of an image that are considered likely to contain an object. The actual
object proposal task can be performed by many algorithms. These object proposals are then passed to an Object
Classification algorithms that classifies each region as an object (typically with some sort of confidence). Object Proposal
and Object Classification used to be considered as two seprate tasks, requiring a dedicated algorithm for each. However,
more recent Object Detection methods incorporate both the Object Proposal and Object Detection tasks into a single
deep neural network architecture, thus creating a single network that can perform the full end to end Object Detection.</p>

<p><strong>Object Proposals and Classification</strong>
<img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/obj_proposals.png" alt="Object Proposals and Classification" title="Object Proposals and Classification" /></p>

<h5 id="deep-object-detection-network-sensitivity">Deep Object Detection Network Sensitivity</h5>

<p>There has been increasing interest in deep neural network based Object Detection method&rsquo;s sensitivity to errors or
variations in images. Currently, most of the focus is in the context of &ldquo;Adverserial Attacks&rdquo;, in which an image is specifically
designed to trick a deep neural network into misclassifying the image with high confidence. In this scenario, some deep
neural network repeatedly probes an Object Detection network with an image, iteratively modifying it to a small degree
until the deep neural network misclassifies it. In some cases, the modifications that need to be made to an image
to trick a deep neural network are so small that they are not visible to the human eye. This issue is visually depicted
below.</p>

<p><strong>Adverserial Object Detection</strong>
<img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/obj_detection_adverserial.png" alt="Adverserial Object Detection" title="Adverserial Object Detection" /></p>

<p>Another kind of deep neural network sensitivity is a deep neural network&rsquo;s ability to correctly detect and classify
objects in the presence of noise, or what we call &ldquo;natural&rdquo; image variation. We make this particular distinction
from the adverserial case because these kinds of image variations are not mathematically constructed for
the purpose of tricking a deep neural network. These are images that may have saturation, contrast, lighting,
noise, blur, and other kinds of issues that may be naturally occuring. From our searches, we have not  been
able to find anyone that has previously proposed this kind of distinction for image variation. In our project, we
focus on these types of &ldquo;natural&rdquo; image variations, and analyse the sensitivity of deep neural networks to this
kind of image variation. A visual representation of some of examples of natural image variations are given
below.</p>

<p><strong>Natural Image Variation Object Detection</strong>
<img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/obj_detection_natural_variation.png" alt="Natural Image Variation Object Detection" title="Natural Image Variation Object Detection" /></p>

<h2 id="project-proposal">Project Proposal</h2>

<h5 id="problem">Problem?</h5>

<p>For this project, we focused on 3 main questions related to deep neural network based object detection methods.</p>

<ol>
<li><p>How sensitive are cutting edge deep object detection networks to natural noise/errors/variation in images?</p></li>

<li><p>What kind of process and metrics should be used to quantify this sensitivity?</p></li>

<li><p>Can our analysis be used to provide a context of which networks to use in different contexts?</p></li>
</ol>

<h5 id="importance">Importance?</h5>

<ol>
<li><p><strong>Safety Applications</strong>: A practical understanding or methodology for determining how Deepnets fail is crucial if we expect to use deep nets in safety concerned applications, such as autonomous driving.</p></li>

<li><p><strong>Context Specific Applications</strong>: Field focus is on “new” networks.
Little knowledge of practical use “in the wild”.</p></li>

<li><p><strong>Lack of Rich Performance Metrics</strong>: Standard ML metrics don’t map very well to object detection.
There aren’t rich metrics that quantify things like “color sensitivity”</p></li>
</ol>

<h5 id="state-of-the-art">State of the art?</h5>

<p>We aim to benchmark some of the most commonly used, modern deep neural networks
including Mask R-CNN , RetinaNet , Faster R-CNN , RPN , Fast R-CNN , and R-FCN . Some of the
pre-trained backbone networks we are looking to use include ResNeXt{50,101,152} ,
ResNet{50,101,152} , Feature Pyramid Networks , and VGG16 . The datasets we aim to use
include the COCO dataset, PASCAL VOC dataset, and the Kitti object detection dataset. These
network architecture designs, and network backbones represent some of the most common
cutting edge object detection deep neural networks that exist today.</p>

<p>In terms of actual analysis, not much work has been performed into analysis of deep
neural network models other than basic average performance metrics of individual models. Most
work focuses more on proposing a new architecture for a deep network object detection model,
rather than comparing its significant difference with existing methods, major fundamental
drawbacks, and other ways in which the network can be fooled or broken. We provide a general
summary of some of the state of the art networks, backbones, data sets, and kinds of analysis.</p>

<ul>
<li>Networks:

<ul>
<li>Mask R-CNN, RetinaNet, Faster R-CNN , RPN , Fast R-CNN , R-FCN, YOLO</li>
</ul></li>
<li>Backbones:

<ul>
<li>ResNeXt{50,101,152} , ResNet{50,101,152} , Feature Pyramid Networks , and VGG16</li>
</ul></li>
<li>Datasets:

<ul>
<li>COCO (Multi-object Detection Dataset)</li>
<li>PASCAL VOC (Multi-object Detection Dataset))</li>
<li>Kitti (Autonomous Vehicle Object Detection Dataset)</li>
</ul></li>
<li>Analysis:

<ul>
<li>Simple aggregated performance metrics</li>
<li>Adversarial based analysis</li>
</ul></li>
</ul>

<h5 id="existing-systems-or-new-approach">Existing Systems or New Approach?</h5>

<p>We plan on benchmarking existing neural network designs and implementations that are
representative of current deep neural network object detection based methods, with one
particular goal being a proposal and outline of future research work that should be developed to
address some of the issues we find. This might fall under the category of a “new approach”.
However, we also plan on performing deeper analysis of existing methods than has been
previously performed to provide for a better overall understanding of how well modern methods
compare, how similar and different the methods are, which approaches seem to work the best,
and what shortcoming exist with existing methods.</p>

<h5 id="evaluation-and-results-plan">Evaluation and Results Plan?</h5>

<p>Our project proposal centers around benchmarking and analyzing different deep object
detection networks, so the evaluation phase is really the meat of our project (aside from getting
everything up and running). The details of what we plan on benchmarking, what platforms we
plan to use, what data sets we plan to use, and what metrics we plan to calculate are given in
the “Approach” section below.</p>

<h2 id="approach">Approach</h2>

<h5 id="high-level-steps">High Level Steps</h5>

<ol>
<li>Select the most cutting edge object detection networks.</li>
<li>Perform performance analysis on a base set of images for each network.</li>
<li>Generate images with “natural” errors/noise/artifacts/transforms.</li>
<li>Perform performance analysis on transformed images for each network.</li>
<li>Compile sets of summary metrics organized by object classes, transform kind, and model.</li>
<li>Compare and contrast performance to draw insights about deep network capabilities and
sensitivities based on class, transform kind, and model.</li>
<li>Create guidelines for developing and using deep object detection networks, and propose new data.</li>
</ol>

<h5 id="networks-images-datasets">Networks, Images, Datasets</h5>

<p>Below, we outline the networks we tested, the analysis we performed, and the systems we used.</p>

<ul>
<li><p>10 Networks (Requires 11GB Graphics Cards):</p>

<ul>
<li>&lt;Net Arch&gt;_&lt;Net Backbone&gt;-&lt;Proposals&gt;_2x</li>
<li>e2e_faster_rcnn_R-101-FPN_2x</li>
<li>e2e_faster_rcnn_R-50-C4_2x</li>
<li>e2e_faster_rcnn_R-50-FPN_2x</li>
<li>e2e_faster_rcnn_X-101-64x4d-FPN_2x</li>
<li>e2e_mask_rcnn_R-101-FPN_2x</li>
<li>e2e_mask_rcnn_R-50-C4_2x</li>
<li>e2e_mask_rcnn_R-50-FPN_2x</li>
<li>retinanet_R-101-FPN_2x</li>
<li>retinanet_R-50-FPN_2x</li>
<li>retinanet_X-101-64x4d-FPN_2x</li>
</ul></li>

<li><p>Images:</p>

<ul>
<li>COCO image validation data set.</li>
<li>5000 base images (600x800 3 channel images)</li>
<li>50 Transforms generating 250,000 images total</li>
<li>5000*50<em>10</em>(300 proposals/net) = 750 Million object candidates</li>
</ul></li>

<li><p>Systems:</p>

<ul>
<li>8 GTX 1080ti GPU system with 11GB per Card, 8 GB Ram</li>
<li>i7 CPU, 24GB Ram</li>
<li>University of Wisconsin Condor GPU Instances</li>
</ul></li>
</ul>

<h5 id="image-transforms">Image Transforms</h5>

<p>We have 50 image transforms that we applied to the COCO validation image set. There
are roughly two categories of image transforms that we applied. The first is value based
transforms. Most of these transforms deal with changing images in ways related to the
value of pixels in an image. The second is spatial based transforms, in which we transform
images in ways that may alter the phisicaly size, shape, or arraingment of pixels within an
image. We provide some visual examples of each kind of image transform below.</p>

<p><strong>Value Based Transforms</strong>
<img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/value_transforms.png" alt="Value Based Transforms" title="Value Based Transforms" /></p>

<p><strong>Spatially Based Transforms</strong>
<img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/spatial_transforms.png" alt="Spatially Based Transforms" title="Spatially Based Transforms" /></p>

<p><strong>Set of All Transforms</strong></p>

<p>transformNames = [
&lsquo;None&rsquo;,
&lsquo;gaussianblur_1&rsquo;,
&lsquo;gaussianblur_10&rsquo;,
&lsquo;gaussianblur_20&rsquo;,
&lsquo;superpixels_0p1&rsquo;,
&lsquo;superpixels_0p5&rsquo;,
&lsquo;superpixels_0p85&rsquo;,
&lsquo;colorspace_25&rsquo;,
&lsquo;colorspace_50&rsquo;,
&lsquo;averageblur_5_11&rsquo;,
&lsquo;medianblur_1&rsquo;,
&lsquo;sharpen_0&rsquo;,
&lsquo;sharpen_1&rsquo;,
&lsquo;sharpen<em>2&rsquo;,
&lsquo;addintensity</em>-80&rsquo;,
&lsquo;addintensity_80&rsquo;,
&lsquo;elementrandomintensity_1&rsquo;,
&lsquo;multiplyintensity_0p25&rsquo;,
&lsquo;multiplyintensity_2&rsquo;,
&lsquo;contrastnormalization_0&rsquo;,
&lsquo;contrastnormalization_1&rsquo;,
&lsquo;contrastnormalization_2&rsquo;,
&lsquo;elastic_1&rsquo;,
&lsquo;scaled_1p25&rsquo;,
&lsquo;scaled_0p75&rsquo;,
&lsquo;scaled<em>0p5&rsquo;,
&lsquo;scaled</em>(1p25, 1p0)&lsquo;,
&lsquo;scaled<em>(0p75, 1p0)&lsquo;,
&lsquo;scaled</em>(1p0, 1p25)&lsquo;,
&lsquo;scaled<em>(1p0, 0p75)&lsquo;,
&lsquo;translate</em>(0p1, 0p1)&lsquo;,
&lsquo;translate<em>(0p1, -0p1)&lsquo;,
&lsquo;translate</em>(-0p1, 0p1)&lsquo;,
&lsquo;translate<em>(-0p1, -0p1)&lsquo;,
&lsquo;translate</em>(0p1, 0)&lsquo;,
&lsquo;translate<em>(-0p1, 0)&lsquo;,
&lsquo;translate</em>(0, 0p1)&lsquo;,
&lsquo;translate_(0, -0p1)&lsquo;,
&lsquo;rotated_3&rsquo;,
&lsquo;rotated_5&rsquo;,
&lsquo;rotated_10&rsquo;,
&lsquo;rotated_45&rsquo;,
&lsquo;rotated_60&rsquo;,
&lsquo;rotated_90&rsquo;,
&lsquo;flipH&rsquo;,
&lsquo;flipV&rsquo;,
&lsquo;dropout&rsquo;,
]</p>

<h2 id="system">System</h2>

<h5 id="systems-overview">Systems Overview</h5>

<p>Our system processing pipeline consits of a general processing scheme of CPU-&gt;GPU-&gt;CPU. We perform initial
image transfromations on our CPU based system. Then, these images are shipped to a GPU based cluster and
other GPU based systems to perform inference. During inference, a set of json object output files are generated
containing the predicted bounding boxes, classes, and confidence scores over every image. These output
files are then sent back to our CPU based system where we convert bounding boxes and confidence scores
to hard predictions. Once the hard predictions of objects has been made, we use some parallelized CPU
based scripts to generate precision and recall values over all combinations of models, transforms,
and class (10 models * 50 transforms * 88 classes). We then analyze these results in an .ipynb, which can be
seen below in the results section.</p>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/pipeline.png" alt="Pipeline" title="Pipeline" /></p>

<h2 id="results">Results</h2>

<p>In this section we detail the results of our analysis. Below, we show and describe some of the main
plots we created during our analysis. However, many of the tables are descriptive examples of our
analysis are cut down. The full set of results from the IPython notebook can be viewed at the
link below. We discuss the metrics we currently use for performane analysis, and discuss
some of the plots we generate, and how they can be used to gain further insight into the performance of
particular combinations of model type, image tranform type, and class category.</p>

<p><a href="https://dcompgriff.github.io/ML_OD_Benchmarking/publications/Average_Metrics_Analysis.html">IPython Notebook Analysis Results</a></p>

<h5 id="current-metrics">Current Metrics</h5>

<p>Currently, we rely on straight Precision and Recall based metrics to perform analysis. We&rsquo;ve found that most papers
produce either precision only metrics, or metrics such as unweighted mean average precision, which favor
high precision and low recall models (which is somewhat stated in the PASCAL paper and the original
Information Retrieval paper for which mean average precision is based on). We&rsquo;ve also noticed that for many papers,
the numbers presented tend to be only for &ldquo;favorable&rdquo; class categories, and aren&rsquo;t given the proper context
considering class skew and P/R curve distributions. For the most part, we&rsquo;ve found that the results shown in papers
are not good indications of actual network performance, and that the metrics used are more useful for discriminating
between &ldquo;rough average performances&rdquo; over sets of tasks, and should <strong>not</strong> be used for determining network
performance on a specific task.</p>

<ul>
<li>Precision and Recall

<ul>
<li>Precision: How confident are we when the model predicts a dog, that the object is a dog, and not something else?</li>
<li>Recall: How many of the instances within a class are actually found?</li>
</ul></li>
<li>Combinations

<ul>
<li>For each combination of &ldquo;Model, Image Transform Type, Class Category&rdquo;, we generate a single precision and recall value.
We then use OLAP style analysis over these values to aggregate and average performance metrics to get better insight.</li>
</ul></li>
</ul>

<h6 id="precision-recall-histograms">Precision Recall Histograms</h6>

<p>The histograms below represent the precision and recall metrics for all model, transform, and class
 combinations. More specifically, say for the combination of model e2e_faster_rcnn_R-50-C4_2x, a
 transform of &lsquo;gaussianblur_1&rsquo;, and class label &lsquo;person&rsquo;, we measured a precision of 0.714 and a recall
 of 0.277 (Given an Intersection over Union of 0.5). For each of the potential combinations, we generated a
 precision and recall, and plotted all of the measures as histograms. We did this to give us a general idea
 of the overall performance of the deep object detection networks. Our average precision was around 0.53,
 and for some classes was much higher. However, for most classes, recall was 0.25 or below (for some it
 was higher, but this was due to class skew and not <em>reliable</em> performance). The red squares below highlight
 the issues of combinations that had either high precision, or high recall, but most of these combinations
 were due to class skew. As we discuss later on, there tends to be a trend of high precision, low recall
 models for the COCO object detection data set that tends to be obscured by the traditional mean
 average precision metrics that are used to evaluate models. We talk more about these issues later.</p>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/pr_graph.png" alt="Short Summary" /></p>

<h5 id="precision-per-model-transform-class">Precision per (model,transform,class)</h5>

<p>The table below shows a slice of a table describing precision for a specific model (in this case,
 the model is &ldquo;e2e_faster_rcnn_R-50-C4_2x&rdquo;). The columns of the table relate to a specific class
 (except the first column), and each row corresponds to a specific transform over the original
 data set (with the first row &ldquo;None&rdquo; being the original image set). For example, the first precision
 entry in the table 0.694 means that for the e2e_faster_rcnn_R-50-C4_2x model, on the
 original image set, for the person class was 0.694.</p>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/precision_per.png" alt="Short Summary" /></p>

<h5 id="recall-per-model-transform-class">Recall per (model,transform,class)</h5>

<p>The table below shows a slice of a table describing recall for a specific model (in this case,
 the model is &ldquo;e2e_faster_rcnn_R-50-C4_2x&rdquo;). The columns of the table relate to a specific class
 (except the first column), and each row corresponds to a specific transform over the original
 data set (with the first row &ldquo;None&rdquo; being the original image set). For example, the first recall
 entry in the table 0.274 means that for the e2e_faster_rcnn_R-50-C4_2x model, on the
 original image set, for the person class was 0.274.</p>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/recall_per.png" alt="Short Summary" /></p>

<h5 id="precision-difference-per-model-transform-class">Precision difference per (model,transform,class)</h5>

<p>The table below shows a slice of a table describing the difference in model precision
between the original image set, and a specific transformed version of the original
image set. A positive value indicates a performance increase, and a negative value
indicates a performance decrease.</p>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/precision_diff_per.png" alt="Short Summary" /></p>

<h5 id="precision-per-class-transform-avg-over-model">Precision per (class,transform) avg over model</h5>

<p>Each box plot below corresponds to a single transform, and shows the average
precision over all models on a per class basis. The plot below shows the averaged
precision over all models on the original data set.</p>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/precision_per_allmodel_1.png" alt="Short Summary" /></p>

<p>The plot below shows the averaged precision over all models on the gaussian
transform data set with a sigma of 1.</p>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/precision_per_allmodel_2.png" alt="Short Summary" /></p>

<p>The key usefulness of these plots is to compare how each transform affects the
aggregated performance over all models on a per class basis, or in general. For
example, the left plot below corresponds to the original data set, and the right plot
corresponds to the gaussian blurred data set. As you can see, for some classes,
precision increased, and for other classes, the precision decreased.</p>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/precision_per_allmodel_3.png" alt="Short Summary" /></p>

<p>In the plot below, the left plot corresponds to the original data set, and the right
plot corresponds to the gaussian blurred data set with a sigma of 20. As can be
seen from the plot, performance is severely degraded when a large amount
of gaussian blur is added to an image.</p>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/precision_per_allmodel_4.png" alt="Short Summary" /></p>

<h5 id="precision-per-model-avg-over-transform-and-class">Precision per (model) avg over transform and class</h5>

<p>The plot below shows a box plot for precision for each model, averaged over all
transformed image sets and classes. The yellow bar is the median performance, and
the green triangle is the average performance. On average, the precision for the retina net
101-64x4d-FPN model performs better than the other models. However, we can&rsquo;t
conclude anything about raw specific performance since the confidence bounds of
every set of models is nearly the same.</p>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/precision_per_model.png" alt="Short Summary" /></p>

<h5 id="recall-per-model-avg-over-transform-and-class">Recall per (model) avg over transform and class</h5>

<p>The plot below shows a box plot for recall for each model, averaged over all
transformed image sets and classes. The yellow bar is the median performance, and
the green triangle is the average performance. On average, the recall for the faster
rcnn 101-64x4d-FPN model performs better than the other models. However, we can&rsquo;t
conclude anything about raw specific performance since the confidence bounds of
every set of models is nearly the same. We can see that recall is very low across all models.
While there are some models with high recall (which doesn&rsquo;t tell us much since we don&rsquo;t
also see their corresponding recall), these cases are clearly outside of the confidence
bounds for the performance over all model.</p>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/recall_per_model.png" alt="Short Summary" /></p>

<h5 id="average-precision-difference-given-each-transform-category">Average precision difference given each transform category</h5>

<p>Each plot below corresponds to a single model. The bars correspond to the average precision
difference for the model between the original image set, and all image sets in the specified
&ldquo;category&rdquo; or transform, over all classes. The first two bars show the highest categories of
value based transforms and spatial based transforms. As can be seen from the plot,
value based variations in images affect model performance on average more than spatially
based transforms. The rest of the bars show the average precision difference over other
categories of transforms. The set of all plots, as with the other plots shown in this section, can
be viewed from the link to the full analysis outputs link at the beginning of this section. The set
of all categories is given below.</p>

<ul>
<li>valueTransforms = [blurTransforms, regionTransforms, colorTransforms, lightingTransforms]</li>
<li>spatialTransforms = [scalingTransforms, orientationTransforms, translationTransforms, distortionTransforms]</li>
<li>blurTransforms = [ &lsquo;gaussianblur_1&rsquo;,
&lsquo;gaussianblur_10&rsquo;,
&lsquo;gaussianblur_20&rsquo;,
&lsquo;averageblur_5_11&rsquo;,
&lsquo;medianblur_1&rsquo;]</li>
<li>regionTransforms = [ &lsquo;superpixels_0p1&rsquo;,
&lsquo;superpixels_0p5&rsquo;,
&lsquo;superpixels_0p85&rsquo;,
&lsquo;averageblur_5_11&rsquo;]</li>
<li>colorTransforms = [&lsquo;colorspace_25&rsquo;,
&lsquo;colorspace_50&rsquo;,
&lsquo;multiplyintensity_0p25&rsquo;,
&lsquo;multiplyintensity_2&rsquo;,
&lsquo;contrastnormalization_0&rsquo;,
&lsquo;contrastnormalization_1&rsquo;,
&lsquo;contrastnormalization_2&rsquo;]</li>
<li>lightingTransforms = [&lsquo;sharpen_0&rsquo;,
&lsquo;sharpen_1&rsquo;,
&lsquo;sharpen<em>2&rsquo;,
&lsquo;addintensity</em>-80&rsquo;,
&lsquo;addintensity_80&rsquo;,
&lsquo;elementrandomintensity_1&rsquo;]</li>
<li>scalingTransforms = [ &lsquo;scaled_1p25&rsquo;,
&lsquo;scaled_0p75&rsquo;,
&lsquo;scaled<em>0p5&rsquo;,
&lsquo;scaled</em>(1p25, 1p0)&lsquo;,
&lsquo;scaled<em>(0p75, 1p0)&lsquo;,
&lsquo;scaled</em>(1p0, 1p25)&lsquo;,
&lsquo;scaled_(1p0, 0p75)&lsquo;]</li>
<li>orientationTransforms = [ &lsquo;rotated_3&rsquo;,
&lsquo;rotated_5&rsquo;,
&lsquo;rotated_10&rsquo;,
&lsquo;rotated_45&rsquo;,
&lsquo;rotated_60&rsquo;,
&lsquo;rotated_90&rsquo;,
&lsquo;flipH&rsquo;,
&lsquo;flipV&rsquo;]</li>
<li>translationTransforms = [&lsquo;translate<em>(0p1, 0p1)&lsquo;,
&lsquo;translate</em>(0p1, -0p1)&lsquo;,
&lsquo;translate<em>(-0p1, 0p1)&lsquo;,
&lsquo;translate</em>(-0p1, -0p1)&lsquo;,
&lsquo;translate<em>(0p1, 0)&lsquo;,
&lsquo;translate</em>(-0p1, 0)&lsquo;,
&lsquo;translate<em>(0, 0p1)&lsquo;,
&lsquo;translate</em>(0, -0p1)&lsquo;]</li>
<li>distortionTransforms = [&lsquo;elastic_1&rsquo;,
&lsquo;dropout&rsquo;]</li>
</ul>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/precision_avg_diff_per_category.png" alt="Short Summary" /></p>

<h2 id="discussion">Discussion</h2>

<h5 id="short-summary">Short Summary</h5>

<p><img src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/short_summary_meme.png" alt="Short Summary" /></p>

<h5 id="long-summary">Long Summary</h5>

<h6 id="issues">Issues</h6>

<p>1) Most networks are super brittle and need more work to make them robust.</p>

<ul>
<li>Any image variation impacts performance.</li>
<li>Significant sensitivity to training image set.</li>
</ul>

<p>2) Most evaluation metrics are average metrics.</p>

<ul>
<li>Weighted average is disingenuous for performance across multiple classes.</li>
<li>There should be a diminishing marginal benefit performance metric.</li>
<li>Precision only (which most papers report) covers up low recall.</li>
<li>Recall IoU, only shows recall profile, not coupled recall  and precision.</li>
<li>The way in which multi-class P/R averages are generated is not well specified.</li>
</ul>

<h5 id="real-world-guide">Real World Guide</h5>

<p>1) Blur</p>

<ul>
<li>Well defined boundary objects (cars, trains, bus, bench, etc…) small blur is bad</li>
<li>Less well defined boundary objects, or moving objects with more variety (people, dogs, birds, etc…) small blur tends to be good.</li>
</ul>

<p>2) Image size</p>

<ul>
<li>Applications with large objects are better. In fact, if dealing with small objects, artificially scale image size to increase network performance.</li>
</ul>

<p>3) Add contrast to your images.</p>

<ul>
<li>Networks are very sensitive to the magnitude of relative intensity differences in images. Amplify these differences for improved performance.</li>
</ul>

<p>4) Locality based jitter and noise is really bad.</p>

<ul>
<li>Severely decreases performance. (Brownian motion type deal)</li>
</ul>

<p>5) Center objects in images.</p>

<ul>
<li>More centered objects are detected more often and more precisely than objects near edges.</li>
</ul>

<p>6) Region textures matter in an object.</p>

<ul>
<li>Superpixel transforms show degraded performance for larger contiguous regions with same color and no textures.</li>
</ul>

<p>7) Hues and alternate color spaces</p>

<ul>
<li>Adjusting hue values up increased performance on average. (Color contrasting affects)</li>
</ul>

<h2 id="project-next-steps">Project Next Steps</h2>

<h5 id="next-steps-for-this-project">Next Steps for This Project?</h5>

<h6 id="sensitivity-analysis-toolkit">Sensitivity Analysis Toolkit.</h6>

<p>In our extensive literature review, we have not found any major initiatives or projects that are
focused on analyzing and quantifying the sensetivity of an object detection network
to natural variation in images. We believe that there would be value in working towards
trying to create a toolkit for testing and measuring object detection sensetivity for
both research and practical applications.</p>

<h6 id="new-sensitivity-metrics">New Sensitivity Metrics.</h6>

<p>Another thing that we&rsquo;ve noticed in our literature review was that many of the performance
metrics that are used for measuring deep object detection networks come from traditional
machine learning and information retrieval areas. However, rich performance metrics
related to the image object detection context are lacking. Sensitivity metrics, like the
ones we&rsquo;ve showed above related to kinds of variation are benneficial. Other rich
metrics could also be feasibly constructed such as objectness variability for specific
classes. New sensitivity metrics such as these would provide better insight for developing
and applying deep object detection networks, which is currently ambiguous and undirected.</p>

<h5 id="next-steps-for-this-area">Next Steps for This Area?</h5>

<ul>
<li>Amount of re-training needed to reduce errors?</li>
<li>Architectural changes/constraints needed to reduce errors?</li>
<li>How much reducing network size for memory/speed affects errors?</li>
</ul>

<p>Notes
Code on Github
4 days to run on our hardware (parallelize by 8)
4*8=32 days to run on single GPU single core system</p>

<h2 id="special-thanks">Special Thanks</h2>

<p>Special thanks to Matt Nichols <a href="//github.com/mattuyw" class="icon-github" target="_blank" title="Github"></a> for providing us access to his 8 GPU bitcoin mining machine!
Without access to such a powerful system, this project would not have been possible.</p>

<h2 id="appendix">Appendix</h2>

<h5 id="challenges-we-faced">Challenges We Faced</h5>

<p>Refactoring our project and proposal.</p>

<ul>
<li>So we had to rapidly change our direction when we realized the tedious
complexity of our original project proposal, and search for a new potential project.
We had to review lots of object detection papers and methods to come up with a
new project to work on. Once we had an idea that we wanted to try to perform a
set of benchmarking, we had to find candidate models, platforms, frameworks,
datasets, and metrics that we thought we could achieveably use for our project.
Some of the challenges of these are found in the next subsections.</li>
</ul>

<p>Running on Condor GPUs and getting access to resources.</p>

<ul>
<li>Most of today’s existing deep neural network, object detection based models
require GPUs (A major drawback which I believe will eventually be remedied with
FPGAs due to their high speed, low power nature which makes them a defacto
choice for most industry scale, long term use). So, we talked to the University of
Wisconsin’s HPC group about running on the GPU based condor instances. We
spent a good amount of time writing the submission and monitoring scripts for the
GPU instances, and in particular the docker based GPU instances to simplify
code submission. However, there are only 2 machines in HPC with 4 total GPUs
that currently run docker, and the queue is so congested that using condor for
our project became clearly impractical (I submitted a GPU based docker job
about a week ago that still hasn’t been run yet). I’ve talked with the condor HPC
Team Members - Daniel Griffin, Yudhister Satija
staff, and they are working on getting docker running on the other 2 older
instances (each with 6-8 GPUs each) that don’t have Docker (But we can’t use
these without docker, as the nature of the code we are trying to run on the
machines requires installation and system configuration permissions).</li>
</ul>

<p>Running on AWS, and the cost associated with it.</p>

<ul>
<li>Since we couldn’t run on the condor instances, we decided to turn to Amazon
Web Services (AWS) to try to provision instances. After a few days of working
with AWS EC2 instances, we finally developed a process for provisioning AWS
GPU instance resources for running our nvidia-docker based code, and running
some of our initial deep net object detection models (Many of which need GPUs
for their operators, and up to 15GB of dedicated system memory). The downside
with this is that AWS GPU instances are $1/HR to provision and use which is
quite expensive for compute resources (And why we are trying to avoid training
models that can each take anywhere from hours to days to train).</li>
</ul>

<p>Dataset non-uniformity.</p>

<ul>
<li>While there exist some different datasets for benchmarking object detection,
most data sets have different formats. We have decided to try to standardize to
the COCO dataset API, and to try to convert some of the other data sets we
would like to use to the COCO annotation specifications. This means that we will
likely have to write scripts for generating COCO annotated images from each
data set.</li>
</ul>

<h3 id="initial-proposal-and-midterm-reports">Initial Proposal and Midterm Reports.</h3>

<ul>
<li><a href="https://dcompgriff.github.io/ML_OD_Benchmarking/publications/766_CV_Project_Proposal.pdf">Initial Project Proposal</a></li>
<li><a href="https://dcompgriff.github.io/ML_OD_Benchmarking/publications/766_CV_Midterm_Report.pdf">Midterm Report</a></li>
<li><a href="https://dcompgriff.github.io/ML_OD_Benchmarking/publications/766_Final_Project_Presentation.pptx">Project Presentation</a></li>
</ul>

            <ul class="share-buttons">
    <li>Share this article:</li>
    <li>
        <a class="icon-facebook-squared" href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fdcompgriff.github.io%2fML_OD_Benchmarking%2fpost%2fdeep-object-detection-benchmarking%2f" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook"></a>
    </li>
    <li>
        <a class="icon-twitter" href="https://twitter.com/share?text=Deep%20object%20detection%20network%20benchmarking%20and%20sensitivity%20analysis.&amp;url=https%3a%2f%2fdcompgriff.github.io%2fML_OD_Benchmarking%2fpost%2fdeep-object-detection-benchmarking%2f" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Tweet this article"></a>
    </li>
    <li>
        <a class="icon-gplus" href="https://plus.google.com/share?url=https%3a%2f%2fdcompgriff.github.io%2fML_OD_Benchmarking%2fpost%2fdeep-object-detection-benchmarking%2f" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google&#43;"></a>
    </li>
    <li>
        <a class="icon-linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdcompgriff.github.io%2fML_OD_Benchmarking%2fpost%2fdeep-object-detection-benchmarking%2f&title=Deep%20object%20detection%20network%20benchmarking%20and%20sensitivity%20analysis." onclick="window.open(this.href, 'linkedin-share', 'width=600,height=494');return false;" title="Share on Linkedin"></a>
    </li>
</ul>

        </article>
        
    </main>
    <aside class="author">
  <img class="profile-image" src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/dgriffin.jpg" alt="Dan Griffin" width="200" height="200"/>
  <p class="name">by 
  <strong>Dan Griffin</strong></p>
  <p class="link"></p>
  <ul class="social">
    












<li><a href="//github.com/dcompgriff" class="icon-github" target="_blank" title="Github"></a></li>




<li><a href="https://dcompgriff.github.io/ML_OD_Benchmarkingpost/index.xml" class="icon-rss" target="_blank" title="RSS"></a></li>

  </ul>
  <br><br>
</aside>
<aside class="author">
    <img class="profile-image" src="https://dcompgriff.github.io/ML_OD_Benchmarking/img/YudhisterSatija.jpg" alt="Yudhister Satija" width="200" height="200"/>
    <p class="name">by
    <strong>Yudhister Satija</strong></p>
    <p class="link"></p>
    <br><br>
</aside>

</div>

<footer class="main-footer">
  <div class="container clearfix">
        <a class="icon-rss" href="https://dcompgriff.github.io/ML_OD_Benchmarking/post/index.xml" title="RSS"></a>
        <p>&copy; 2018 &middot; Powered by <a href="http://gohugo.io">Hugo</a>.</p>
  </div>
</footer>


<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script> 
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>window.jQuery || document.write('<script src="https:\/\/dcompgriff.github.io\/ML_OD_Benchmarking\/js\/vendor\/jquery-1.11.0.min.js"><\/script>')</script>
<script src="https://dcompgriff.github.io/ML_OD_Benchmarking/js/plugins.js"></script>




</body>
</html>

